<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation - BSP</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <nav>
        <a href="index.html" class="logo">BSP Docs</a>
        <ul>
            <li><a href="architecture/index.html">Architecture</a></li>
            <li><a href="theory/index.html">Theory</a></li>
            <li><a href="research/index.html">Research</a></li>
            <li><a href="wiki/index.html">Wiki & Glossary</a></li>
            <li><a href="specs/index.html">Specs</a></li>
            <li><a href="training.html">Training</a></li>
            <li><a href="eval.html" class="active">Evaluation</a></li>
        </ul>
    </nav>

	    <div class="container">
	        <main class="content">
	            <h1>Evaluation</h1>
	            <p>
	                BSP evaluation is split into two tracks:
	                <strong>Discovery Evals</strong> (synthetic, for architecture discovery) and
	                <strong>Comparative Benchmarks</strong> (dataset-based, for positioning).
	            </p>

	            <div class="callout">
	                <h4>Two Evaluation Tracks</h4>
	                <p>
	                    <strong>Discovery:</strong> learning curves, time-to-threshold, and diagnostic signals on synthetic primitives.<br>
	                    <strong>Comparative:</strong> BPC proxy, surprise rate, and dataset-based reasoning proxies.<br>
	                    <strong>Efficiency:</strong> throughput and memory growth (groups/edges).
	                </p>
	            </div>

	            <h2>1) Discovery Evals (Abstract Primitives)</h2>
	            <p>
	                The Abstract Primitives suite is synthetic and is used to stress-test specific capabilities and
	                measure learning speed and stability.
	            </p>
	            <pre><code># Generate datasets (deterministic)
	node evals/abstract_primitives/generate.mjs --all --seed=1

	# Train+evaluate and save results
	node evals/abstract_primitives/evaluate.mjs --all --seed=1

	# Generate a static HTML dashboard
	node evals/abstract_primitives/dashboard.mjs --seed=1
	</code></pre>
	            <p>
	                Results are saved under <code>evals/abstract_primitives/results/seed_1/</code>.
	            </p>

	            <h2>2) Comparative Benchmarks (Positioning)</h2>
	            <p>
	                Comparative benchmarks run on real text datasets to track progress and keep reference baselines.
	            </p>
	            <pre><code># One-command comparative run (downloads datasets if available, otherwise uses fallback data)
	node evals/runLM_Comp.mjs

	# Evaluate a trained model on any available datasets under data/
	node scripts/evaluate.mjs --model data/model_wiki.json
	</code></pre>

	            <h3>Reference Benchmarks</h3>
	            <ul>
	                <li><strong>WikiText-2</strong>: common LM benchmark (perplexity for LMs, BPC proxy for BSP).</li>
	                <li><strong>LAMBADA</strong>: long-range dependency / cloze reasoning.</li>
            </ul>

	            <p>
	                For the full benchmarking plan and GPT-2 reference targets, see
	                <a href="specs/DS/DS-008-benchmarks-comparison.md">DS-008</a>.
	            </p>

            <h2>Future Benchmark Extensions (Small-LLM Style)</h2>
            <p>
                To compare BSP with “small LLM” suites more directly, we can add:
            </p>
            <ul>
                <li>More cloze and multi-hop reasoning tasks (see <a href="specs/DS/DS-016-reasoning-curriculum-and-data.md">DS-016</a>).</li>
                <li>Task-format and instruction-following evaluation sets (see <a href="specs/DS/DS-015-conversational-training-data.md">DS-015</a>).</li>
                <li>Report generation dashboards (planned in <a href="specs/DS/DS-008-benchmarks-comparison.md">DS-008</a>).</li>
            </ul>
        </main>
    </div>
</body>
</html>
